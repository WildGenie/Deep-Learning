{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing as pre\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Iris Dataset and convert to ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Remove the String labels from the last column (4)\n",
    "for row in dataset:\n",
    "    if row[4] == 'Iris-setosa':\n",
    "        row[4] = 0.0\n",
    "    elif row[4] == 'Iris-versicolor':\n",
    "        row[4] = 1.0\n",
    "        \n",
    "# Select features 0 and 2\n",
    "features_combination = [0, 2]\n",
    "\n",
    "# Crop Database\n",
    "dataset = dataset[:100]\n",
    "\n",
    "# Shuffle samples\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Create labels array from the last column of dataset\n",
    "labels = dataset[:, -1].astype(np.float64)\n",
    "\n",
    "# Normalize dataset\n",
    "norm_dataset = pre.minmax_scale(dataset[:, features_combination])\n",
    "\n",
    "# Select 80-20 training_set, test_set analogies\n",
    "training_set = norm_dataset[:80]\n",
    "training_labels = labels[:80]\n",
    "test_set = norm_dataset[-20:]\n",
    "test_labels = labels[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.4\n",
    "regularization_term = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, training_set, labels, learning_rate, regularization_term):\n",
    "        self.training_set = training_set\n",
    "        self.labels = labels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_term = regularization_term\n",
    "\n",
    "        input_size = self.training_set.shape\n",
    "\n",
    "        # Add bias\n",
    "        # self.input_layer = np.c_[np.ones(input_size[0]), training_set]\n",
    "        # No bias\n",
    "        self.input_layer = training_set\n",
    "\n",
    "        # self.input_weights = np.random.uniform(0, 0.2, input_size[1] + 1)\n",
    "        # No bias\n",
    "        self.input_weights = np.random.uniform(0, 0.2, input_size[1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        np.random.shuffle(input)\n",
    "        self.z = np.zeros(input.shape)\n",
    "        self.z = np.dot(input, self.input_weights)\n",
    "        prediction = np.array(sigmoid(self.z), dtype=float)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def cost_function(self, prediction):\n",
    "        cost = 0\n",
    "        sum_errors = 0\n",
    "        for idx, pred in enumerate(prediction):\n",
    "            if self.labels[idx] == 1:\n",
    "                cost = - pred * np.log(pred + 0.000005)\n",
    "\n",
    "            elif self.labels[idx] == 0:\n",
    "                cost = - (1 - pred) * np.log(1 - pred + 0.000005)\n",
    "            sum_errors += cost\n",
    "        # return sum_errors / len(prediction)\n",
    "        return sum_errors\n",
    "\n",
    "    def backpropagate(self, predictions):\n",
    "        # gradient = np.dot(np.dot(self.labels, (1 / predictions)) - np.dot(1 - self.labels, 1 / (1 - predictions)),\n",
    "        #                   sigmoid_derivate(self.z))\n",
    "        # self.gradients = np.ones_like(self.input_weights)\n",
    "        self.gradients = np.zeros(self.input_weights.shape)\n",
    "        self.gradients = np.dot((self.labels - predictions), self.input_layer)\n",
    "\n",
    "    def gradient_descend(self):\n",
    "        # Update the weights based on the learning rate\n",
    "        # for weight in range(len(self.input_weights)):\n",
    "        #     self.input_weights[weight] = self.input_weights[weight] - self.learning_rate * self.gradients[weight]\n",
    "        self.input_weights = self.input_weights - (self.learning_rate * self.gradients)\n",
    "\n",
    "    def train_network(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(self.input_layer)\n",
    "            cost = self.cost_function(predictions)\n",
    "            self.backpropagate(predictions)\n",
    "            self.gradient_descend()\n",
    "\n",
    "            print(\"Epoch: {0}, Cost: {1}\".format(epoch, cost))\n",
    "\n",
    "    def test_network(self, test_set):\n",
    "        return self.forward(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 27.541836059873386\n",
      "Epoch: 1, Cost: 26.0892384991768\n",
      "Epoch: 2, Cost: 10.591505958669426\n",
      "Epoch: 3, Cost: 2.7106437129553784\n",
      "Epoch: 4, Cost: 1.3800730143548305\n",
      "Epoch: 5, Cost: 0.5172008453074723\n",
      "Epoch: 6, Cost: 0.32304097488762606\n",
      "Epoch: 7, Cost: 0.1502416962455585\n",
      "Epoch: 8, Cost: 0.08711216422733027\n",
      "Epoch: 9, Cost: 0.009309270142099313\n",
      "Epoch: 10, Cost: 0.018280899777270203\n",
      "Epoch: 11, Cost: 0.0018506488608730168\n",
      "Epoch: 12, Cost: 0.002320951704454065\n",
      "Epoch: 13, Cost: 0.0017548258959233225\n",
      "Epoch: 14, Cost: 0.00039010077332654\n",
      "Epoch: 15, Cost: 0.000285715752404807\n",
      "Epoch: 16, Cost: -5.986567904616518e-05\n",
      "Epoch: 17, Cost: -0.00013991056537055471\n",
      "Epoch: 18, Cost: -0.0001538672728470725\n",
      "Epoch: 19, Cost: -0.00017756097488852998\n",
      "Epoch: 20, Cost: -0.00019856194093233643\n",
      "Epoch: 21, Cost: -0.0002020889316805126\n",
      "Epoch: 22, Cost: -0.00020256746372785504\n",
      "Epoch: 23, Cost: -0.000203769301159706\n",
      "Epoch: 24, Cost: -0.00020470389177439436\n",
      "Epoch: 25, Cost: -0.00020486379054334938\n",
      "Epoch: 26, Cost: -0.00020488576391912045\n",
      "Epoch: 27, Cost: -0.00020494860301633401\n",
      "Epoch: 28, Cost: -0.00020497561833802047\n",
      "Epoch: 29, Cost: -0.00020499321164407322\n",
      "Epoch: 30, Cost: -0.00020499672255864176\n",
      "Epoch: 31, Cost: -0.00020499757408820192\n",
      "Epoch: 32, Cost: -0.0002049986257465039\n",
      "Epoch: 33, Cost: -0.00020499927446806764\n",
      "Epoch: 34, Cost: -0.0002049993205401216\n",
      "Epoch: 35, Cost: -0.0002049994791155383\n",
      "Epoch: 36, Cost: -0.00020499947076036695\n",
      "Epoch: 37, Cost: -0.00020499947250333605\n",
      "Epoch: 38, Cost: -0.00020499948063874032\n",
      "Epoch: 39, Cost: -0.00020499948274648773\n",
      "Epoch: 40, Cost: -0.00020499948534836863\n",
      "Epoch: 41, Cost: -0.00020499948715074014\n",
      "Epoch: 42, Cost: -0.00020499948746463748\n",
      "Epoch: 43, Cost: -0.00020499948734821195\n",
      "Epoch: 44, Cost: -0.00020499948740277036\n",
      "Epoch: 45, Cost: -0.00020499948745155563\n",
      "Epoch: 46, Cost: -0.00020499948747594826\n",
      "Epoch: 47, Cost: -0.0002049994874946983\n",
      "Epoch: 48, Cost: -0.00020499948749492033\n",
      "Epoch: 49, Cost: -0.0002049994875030512\n",
      "Epoch: 50, Cost: -0.0002049994875030512\n",
      "Epoch: 51, Cost: -0.0002049994875030512\n",
      "Epoch: 52, Cost: -0.0002049994875030512\n",
      "Epoch: 53, Cost: -0.0002049994875030512\n",
      "Epoch: 54, Cost: -0.0002049994875030512\n",
      "Epoch: 55, Cost: -0.0002049994875030512\n",
      "Epoch: 56, Cost: -0.0002049994875030512\n",
      "Epoch: 57, Cost: -0.0002049994875030512\n",
      "Epoch: 58, Cost: -0.0002049994875030512\n",
      "Epoch: 59, Cost: -0.0002049994875030512\n",
      "Epoch: 60, Cost: -0.0002049994875030512\n",
      "Epoch: 61, Cost: -0.0002049994875030512\n",
      "Epoch: 62, Cost: -0.0002049994875030512\n",
      "Epoch: 63, Cost: -0.0002049994875030512\n",
      "Epoch: 64, Cost: -0.0002049994875030512\n",
      "Epoch: 65, Cost: -0.0002049994875030512\n",
      "Epoch: 66, Cost: -0.0002049994875030512\n",
      "Epoch: 67, Cost: -0.0002049994875030512\n",
      "Epoch: 68, Cost: -0.0002049994875030512\n",
      "Epoch: 69, Cost: -0.0002049994875030512\n",
      "Epoch: 70, Cost: -0.0002049994875030512\n",
      "Epoch: 71, Cost: -0.0002049994875030512\n",
      "Epoch: 72, Cost: -0.0002049994875030512\n",
      "Epoch: 73, Cost: -0.0002049994875030512\n",
      "Epoch: 74, Cost: -0.0002049994875030512\n",
      "Epoch: 75, Cost: -0.0002049994875030512\n",
      "Epoch: 76, Cost: -0.0002049994875030512\n",
      "Epoch: 77, Cost: -0.0002049994875030512\n",
      "Epoch: 78, Cost: -0.0002049994875030512\n",
      "Epoch: 79, Cost: -0.0002049994875030512\n",
      "Epoch: 80, Cost: -0.0002049994875030512\n",
      "Epoch: 81, Cost: -0.0002049994875030512\n",
      "Epoch: 82, Cost: -0.0002049994875030512\n",
      "Epoch: 83, Cost: -0.0002049994875030512\n",
      "Epoch: 84, Cost: -0.0002049994875030512\n",
      "Epoch: 85, Cost: -0.0002049994875030512\n",
      "Epoch: 86, Cost: -0.0002049994875030512\n",
      "Epoch: 87, Cost: -0.0002049994875030512\n",
      "Epoch: 88, Cost: -0.0002049994875030512\n",
      "Epoch: 89, Cost: -0.0002049994875030512\n",
      "Epoch: 90, Cost: -0.0002049994875030512\n",
      "Epoch: 91, Cost: -0.0002049994875030512\n",
      "Epoch: 92, Cost: -0.0002049994875030512\n",
      "Epoch: 93, Cost: -0.0002049994875030512\n",
      "Epoch: 94, Cost: -0.0002049994875030512\n",
      "Epoch: 95, Cost: -0.0002049994875030512\n",
      "Epoch: 96, Cost: -0.0002049994875030512\n",
      "Epoch: 97, Cost: -0.0002049994875030512\n",
      "Epoch: 98, Cost: -0.0002049994875030512\n",
      "Epoch: 99, Cost: -0.0002049994875030512\n",
      "Epoch: 100, Cost: -0.0002049994875030512\n",
      "Epoch: 101, Cost: -0.0002049994875030512\n",
      "Epoch: 102, Cost: -0.0002049994875030512\n",
      "Epoch: 103, Cost: -0.0002049994875030512\n",
      "Epoch: 104, Cost: -0.0002049994875030512\n",
      "Epoch: 105, Cost: -0.0002049994875030512\n",
      "Epoch: 106, Cost: -0.0002049994875030512\n",
      "Epoch: 107, Cost: -0.0002049994875030512\n",
      "Epoch: 108, Cost: -0.0002049994875030512\n",
      "Epoch: 109, Cost: -0.0002049994875030512\n",
      "Epoch: 110, Cost: -0.0002049994875030512\n",
      "Epoch: 111, Cost: -0.0002049994875030512\n",
      "Epoch: 112, Cost: -0.0002049994875030512\n",
      "Epoch: 113, Cost: -0.0002049994875030512\n",
      "Epoch: 114, Cost: -0.0002049994875030512\n",
      "Epoch: 115, Cost: -0.0002049994875030512\n",
      "Epoch: 116, Cost: -0.0002049994875030512\n",
      "Epoch: 117, Cost: -0.0002049994875030512\n",
      "Epoch: 118, Cost: -0.0002049994875030512\n",
      "Epoch: 119, Cost: -0.0002049994875030512\n",
      "Epoch: 120, Cost: -0.0002049994875030512\n",
      "Epoch: 121, Cost: -0.0002049994875030512\n",
      "Epoch: 122, Cost: -0.0002049994875030512\n",
      "Epoch: 123, Cost: -0.0002049994875030512\n",
      "Epoch: 124, Cost: -0.0002049994875030512\n",
      "Epoch: 125, Cost: -0.0002049994875030512\n",
      "Epoch: 126, Cost: -0.0002049994875030512\n",
      "Epoch: 127, Cost: -0.0002049994875030512\n",
      "Epoch: 128, Cost: -0.0002049994875030512\n",
      "Epoch: 129, Cost: -0.0002049994875030512\n",
      "Epoch: 130, Cost: -0.0002049994875030512\n",
      "Epoch: 131, Cost: -0.0002049994875030512\n",
      "Epoch: 132, Cost: -0.0002049994875030512\n",
      "Epoch: 133, Cost: -0.0002049994875030512\n",
      "Epoch: 134, Cost: -0.0002049994875030512\n",
      "Epoch: 135, Cost: -0.0002049994875030512\n",
      "Epoch: 136, Cost: -0.0002049994875030512\n",
      "Epoch: 137, Cost: -0.0002049994875030512\n",
      "Epoch: 138, Cost: -0.0002049994875030512\n",
      "Epoch: 139, Cost: -0.0002049994875030512\n",
      "Epoch: 140, Cost: -0.0002049994875030512\n",
      "Epoch: 141, Cost: -0.0002049994875030512\n",
      "Epoch: 142, Cost: -0.0002049994875030512\n",
      "Epoch: 143, Cost: -0.0002049994875030512\n",
      "Epoch: 144, Cost: -0.0002049994875030512\n",
      "Epoch: 145, Cost: -0.0002049994875030512\n",
      "Epoch: 146, Cost: -0.0002049994875030512\n",
      "Epoch: 147, Cost: -0.0002049994875030512\n",
      "Epoch: 148, Cost: -0.0002049994875030512\n",
      "Epoch: 149, Cost: -0.0002049994875030512\n",
      "Epoch: 150, Cost: -0.0002049994875030512\n",
      "Epoch: 151, Cost: -0.0002049994875030512\n",
      "Epoch: 152, Cost: -0.0002049994875030512\n",
      "Epoch: 153, Cost: -0.0002049994875030512\n",
      "Epoch: 154, Cost: -0.0002049994875030512\n",
      "Epoch: 155, Cost: -0.0002049994875030512\n",
      "Epoch: 156, Cost: -0.0002049994875030512\n",
      "Epoch: 157, Cost: -0.0002049994875030512\n",
      "Epoch: 158, Cost: -0.0002049994875030512\n",
      "Epoch: 159, Cost: -0.0002049994875030512\n",
      "Epoch: 160, Cost: -0.0002049994875030512\n",
      "Epoch: 161, Cost: -0.0002049994875030512\n",
      "Epoch: 162, Cost: -0.0002049994875030512\n",
      "Epoch: 163, Cost: -0.0002049994875030512\n",
      "Epoch: 164, Cost: -0.0002049994875030512\n",
      "Epoch: 165, Cost: -0.0002049994875030512\n",
      "Epoch: 166, Cost: -0.0002049994875030512\n",
      "Epoch: 167, Cost: -0.0002049994875030512\n",
      "Epoch: 168, Cost: -0.0002049994875030512\n",
      "Epoch: 169, Cost: -0.0002049994875030512\n",
      "Epoch: 170, Cost: -0.0002049994875030512\n",
      "Epoch: 171, Cost: -0.0002049994875030512\n",
      "Epoch: 172, Cost: -0.0002049994875030512\n",
      "Epoch: 173, Cost: -0.0002049994875030512\n",
      "Epoch: 174, Cost: -0.0002049994875030512\n",
      "Epoch: 175, Cost: -0.0002049994875030512\n",
      "Epoch: 176, Cost: -0.0002049994875030512\n",
      "Epoch: 177, Cost: -0.0002049994875030512\n",
      "Epoch: 178, Cost: -0.0002049994875030512\n",
      "Epoch: 179, Cost: -0.0002049994875030512\n",
      "Epoch: 180, Cost: -0.0002049994875030512\n",
      "Epoch: 181, Cost: -0.0002049994875030512\n",
      "Epoch: 182, Cost: -0.0002049994875030512\n",
      "Epoch: 183, Cost: -0.0002049994875030512\n",
      "Epoch: 184, Cost: -0.0002049994875030512\n",
      "Epoch: 185, Cost: -0.0002049994875030512\n",
      "Epoch: 186, Cost: -0.0002049994875030512\n",
      "Epoch: 187, Cost: -0.0002049994875030512\n",
      "Epoch: 188, Cost: -0.0002049994875030512\n",
      "Epoch: 189, Cost: -0.0002049994875030512\n",
      "Epoch: 190, Cost: -0.0002049994875030512\n",
      "Epoch: 191, Cost: -0.0002049994875030512\n",
      "Epoch: 192, Cost: -0.0002049994875030512\n",
      "Epoch: 193, Cost: -0.0002049994875030512\n",
      "Epoch: 194, Cost: -0.0002049994875030512\n",
      "Epoch: 195, Cost: -0.0002049994875030512\n",
      "Epoch: 196, Cost: -0.0002049994875030512\n",
      "Epoch: 197, Cost: -0.0002049994875030512\n",
      "Epoch: 198, Cost: -0.0002049994875030512\n",
      "Epoch: 199, Cost: -0.0002049994875030512\n",
      "Epoch: 200, Cost: -0.0002049994875030512\n",
      "Epoch: 201, Cost: -0.0002049994875030512\n",
      "Epoch: 202, Cost: -0.0002049994875030512\n",
      "Epoch: 203, Cost: -0.0002049994875030512\n",
      "Epoch: 204, Cost: -0.0002049994875030512\n",
      "Epoch: 205, Cost: -0.0002049994875030512\n",
      "Epoch: 206, Cost: -0.0002049994875030512\n",
      "Epoch: 207, Cost: -0.0002049994875030512\n",
      "Epoch: 208, Cost: -0.0002049994875030512\n",
      "Epoch: 209, Cost: -0.0002049994875030512\n",
      "Epoch: 210, Cost: -0.0002049994875030512\n",
      "Epoch: 211, Cost: -0.0002049994875030512\n",
      "Epoch: 212, Cost: -0.0002049994875030512\n",
      "Epoch: 213, Cost: -0.0002049994875030512\n",
      "Epoch: 214, Cost: -0.0002049994875030512\n",
      "Epoch: 215, Cost: -0.0002049994875030512\n",
      "Epoch: 216, Cost: -0.0002049994875030512\n",
      "Epoch: 217, Cost: -0.0002049994875030512\n",
      "Epoch: 218, Cost: -0.0002049994875030512\n",
      "Epoch: 219, Cost: -0.0002049994875030512\n",
      "Epoch: 220, Cost: -0.0002049994875030512\n",
      "Epoch: 221, Cost: -0.0002049994875030512\n",
      "Epoch: 222, Cost: -0.0002049994875030512\n",
      "Epoch: 223, Cost: -0.0002049994875030512\n",
      "Epoch: 224, Cost: -0.0002049994875030512\n",
      "Epoch: 225, Cost: -0.0002049994875030512\n",
      "Epoch: 226, Cost: -0.0002049994875030512\n",
      "Epoch: 227, Cost: -0.0002049994875030512\n",
      "Epoch: 228, Cost: -0.0002049994875030512\n",
      "Epoch: 229, Cost: -0.0002049994875030512\n",
      "Epoch: 230, Cost: -0.0002049994875030512\n",
      "Epoch: 231, Cost: -0.0002049994875030512\n",
      "Epoch: 232, Cost: -0.0002049994875030512\n",
      "Epoch: 233, Cost: -0.0002049994875030512\n",
      "Epoch: 234, Cost: -0.0002049994875030512\n",
      "Epoch: 235, Cost: -0.0002049994875030512\n",
      "Epoch: 236, Cost: -0.0002049994875030512\n",
      "Epoch: 237, Cost: -0.0002049994875030512\n",
      "Epoch: 238, Cost: -0.0002049994875030512\n",
      "Epoch: 239, Cost: -0.0002049994875030512\n",
      "Epoch: 240, Cost: -0.0002049994875030512\n",
      "Epoch: 241, Cost: -0.0002049994875030512\n",
      "Epoch: 242, Cost: -0.0002049994875030512\n",
      "Epoch: 243, Cost: -0.0002049994875030512\n",
      "Epoch: 244, Cost: -0.0002049994875030512\n",
      "Epoch: 245, Cost: -0.0002049994875030512\n",
      "Epoch: 246, Cost: -0.0002049994875030512\n",
      "Epoch: 247, Cost: -0.0002049994875030512\n",
      "Epoch: 248, Cost: -0.0002049994875030512\n",
      "Epoch: 249, Cost: -0.0002049994875030512\n"
     ]
    }
   ],
   "source": [
    "# Initiate Model\n",
    "logistic_regression = LogisticRegression(training_set=training_set,\n",
    "                                             labels=training_labels,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         regularization_term=regularization_term)\n",
    "\n",
    "logistic_regression.train_network(epochs=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test_pred = logistic_regression.test_network(test_set)\n",
    "\n",
    "p = np.c_[test_labels, test_pred]\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
